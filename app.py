# -*- coding: utf-8 -*-
import os
import io
from datetime import datetime, date

import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st


# -----------------------------
# ë„¤ì´ë²„ ë‰´ìŠ¤ 1ê±´ ìƒì„¸ ê°€ì ¸ì˜¤ê¸°
# -----------------------------
def get_news(n_url: str):
    """
    ê°œë³„ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ
    ì œëª©, ë‚ ì§œ, ë³¸ë¬¸, ì–¸ë¡ ì‚¬, ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    news_detail = {
        "date": "",
        "company": "",
        "title": "",
        "content": "",
        "link": n_url,
    }

    try:
        breq = requests.get(n_url, timeout=5)
        breq.raise_for_status()
    except Exception as e:
        # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ë¹„ì–´ìˆëŠ” ë°ì´í„° ë°˜í™˜
        print(f"ìš”ì²­ ì‹¤íŒ¨: {n_url} / {e}")
        return news_detail

    bsoup = BeautifulSoup(breq.content, "html.parser")

    # ì œëª© ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡° ë³€ê²½ ëŒ€ì‘)
    title_el = (
        bsoup.select_one("h2#title_area")  # ìƒˆ êµ¬ì¡°
        or bsoup.select_one("h3#articleTitle")  # ì˜ˆì „ êµ¬ì¡°
    )
    if title_el:
        news_detail["title"] = title_el.get_text(strip=True)

    # ë‚ ì§œ ì¶”ì¶œ
    # ìƒˆ êµ¬ì¡°: span.media_end_head_info_datestamp_time / ì˜› êµ¬ì¡°: .t11
    pdate_el = (
        bsoup.select_one("span.media_end_head_info_datestamp_time")
        or bsoup.select_one(".t11")
    )
    if pdate_el:
        news_detail["date"] = pdate_el.get_text(strip=True)[:16]

    # ë³¸ë¬¸ ì¶”ì¶œ
    body_el = (
        bsoup.select_one("article#dic_area")  # ìƒˆ êµ¬ì¡°
        or bsoup.select_one("#articleBodyContents")  # ì˜› êµ¬ì¡°
    )
    if body_el:
        text = body_el.get_text(" ", strip=True)
        # í”Œë˜ì‹œ ìš°íšŒ ë¬¸êµ¬ ì œê±°(ì˜ˆì „ êµ¬ì¡°)
        text = text.replace(
            "// flash ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ function _flash_removeCallback() {}",
            "",
        )
        news_detail["content"] = text.strip()

    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ (ìƒˆ/êµ¬ì¡° í˜¼í•© ëŒ€ì‘)
    company_el = (
        bsoup.select_one("a.media_end_head_top_logo")  # ìƒˆ êµ¬ì¡°
        or bsoup.select_one("#footer address a")  # ì˜ˆì „ êµ¬ì¡°
    )
    if company_el:
        news_detail["company"] = company_el.get_text(strip=True)

    return news_detail


# -----------------------------
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ëŸ¬
# -----------------------------
def crawler(max_page: int, query: str, s_date: str, e_date: str) -> pd.DataFrame:
    """
    max_page: ìµœëŒ€ ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ë‹¹ 10ê±´, ë„¤ì´ë²„ ê¸°ì¤€)
    query   : ê²€ìƒ‰ì–´
    s_date  : ì‹œì‘ì¼ì 'YYYY.MM.DD'
    e_date  : ì¢…ë£Œì¼ì 'YYYY.MM.DD'
    """
    s_from = s_date.replace(".", "")
    e_to = e_date.replace(".", "")

    page = 1
    maxpage_t = (max_page - 1) * 10 + 1  # 1,11,21,...

    results = []

    while page <= maxpage_t:
        url = (
            "https://search.naver.com/search.naver"
            f"?where=news&query={query}"
            f"&sort=0&ds={s_date}&de={e_date}"
            f"&nso=so%3Ar%2Cp%3Afrom{s_from}to{e_to}%2Ca%3A&start={page}"
        )

        print("ìš”ì²­ URL:", url)
        try:
            req = requests.get(url, timeout=5)
            req.raise_for_status()
        except Exception as e:
            print("ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", e)
            page += 10
            continue

        soup = BeautifulSoup(req.content, "html.parser")

        # 1) ìƒˆ êµ¬ì¡°: a.news_tit
        link_tags = soup.select("a.news_tit")
        # 2) êµ¬ êµ¬ì¡°: a._sp_each_url (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        if not link_tags:
            link_tags = soup.select("a._sp_each_url")

        if not link_tags:
            print("ë‰´ìŠ¤ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            break

        for a_tag in link_tags:
            href = a_tag.get("href", "")
            if not href:
                continue
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ë§Œ í¬ë¡¤ë§
            if not href.startswith("https://news.naver.com"):
                continue

            news_detail = get_news(href)
            if news_detail["title"]:  # ì œëª© ì—†ëŠ” ê²½ìš°ëŠ” ìŠ¤í‚µ
                results.append(
                    {
                        "date": news_detail["date"],
                        "company": news_detail["company"],
                        "title": news_detail["title"],
                        "content": news_detail["content"],
                        "link": news_detail["link"],
                    }
                )

        page += 10

    if results:
        df = pd.DataFrame(results)
    else:
        df = pd.DataFrame(columns=["date", "company", "title", "content", "link"])

    return df


# -----------------------------
# Streamlit UI
# -----------------------------
def main():
    st.set_page_config(
        page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
        page_icon="ğŸ“°",
        layout="wide",
    )

    st.title("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì›¹ ì•±")
    st.markdown(
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ íŠ¹ì • **ê²€ìƒ‰ì–´ + ê¸°ê°„**ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³ ,<br>
        ê²°ê³¼ë¥¼ **í™”ë©´ì—ì„œ í™•ì¸**í•˜ê±°ë‚˜ **CSV/ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """,
        unsafe_allow_html=True,
    )

    st.markdown("---")

    # ì…ë ¥ í¼
    with st.form(key="search_form"):
        col1, col2 = st.columns(2)

        with col1:
            query = st.text_input("ê²€ìƒ‰ì–´", value="íŒŒì´ì¬", placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥, ê²½ì œ, ì£¼ì‹ ë“±")
            max_page = st.number_input(
                "ìµœëŒ€ ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ë‹¹ ì•½ 10ê±´)",
                min_value=1,
                max_value=50,
                value=3,
            )

        with col2:
            today = date.today()
            s_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=today.replace(day=1))
            e_date = st.date_input("ë ë‚ ì§œ", value=today)

        submitted = st.form_submit_button("í¬ë¡¤ë§ ì‹œì‘í•˜ê¸° ğŸ•µï¸â€â™‚ï¸")

    if submitted:
        if not query.strip():
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        if s_date > e_date:
            st.error("ì‹œì‘ ë‚ ì§œê°€ ë ë‚ ì§œë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë‚ ì§œ ë¬¸ìì—´ í¬ë§· ë§ì¶”ê¸°
        s_date_str = s_date.strftime("%Y.%m.%d")
        e_date_str = e_date.strftime("%Y.%m.%d")

        with st.spinner("ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            df = crawler(
                max_page=int(max_page),
                query=query,
                s_date=s_date_str,
                e_date=e_date_str,
            )

        if df.empty:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ ë˜ëŠ” ê¸°ê°„ì„ ë³€ê²½í•´ ë³´ì„¸ìš”.")
            return

        st.success(f"ì´ {len(df)}ê°œì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ğŸ“„ í¬ë¡¤ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df, use_container_width=True)

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.markdown("### ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")

        # CSV ë‹¤ìš´ë¡œë“œ
        csv_buffer = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_buffer,
            file_name=f"naver_news_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
        )

        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (openpyxl í•„ìš”)
        try:
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                df.to_excel(writer, index=False, sheet_name="news")
            excel_buffer.seek(0)

            st.download_button(
                label="ì—‘ì…€(xlsx) ë‹¤ìš´ë¡œë“œ",
                data=excel_buffer,
                file_name=f"naver_news_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
        except Exception as e:
            st.info(
                "ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                "requirements.txtì— `openpyxl`ì„ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
            )
            print("Excel export error:", e)


if __name__ == "__main__":
    main()
